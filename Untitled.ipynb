{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa1e4bf0-1e6e-4e8c-9311-25d63bae5e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: / \n",
      "Found conflicts! Looking for incompatible packages.\n",
      "This can take several minutes.  Press CTRL-C to abort.\n",
      "                                                                               failed\n",
      "\n",
      "UnsatisfiableError: The following specifications were found\n",
      "to be incompatible with the existing python installation in your environment:\n",
      "\n",
      "Specifications:\n",
      "\n",
      "  - tensorflow -> python[version='2.7.*|3.7.*|3.6.*|3.5.*']\n",
      "\n",
      "Your python: python=3.9\n",
      "\n",
      "If python is on the left-most side of the chain, that's the version you've asked for.\n",
      "When python appears to the right, that indicates that the thing on the left is somehow\n",
      "not available for the python version you are constrained to. Note that conda will not\n",
      "change your python version to a different minor version unless you explicitly specify\n",
      "that.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1dc07bc-6965-4823-a7a7-0e32f6505380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.7 (default, Sep 16 2021, 08:50:36) \n",
      "[Clang 10.0.0 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26e364bf-3e7d-4c72-9e2e-2826846d9d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np ## For numerical python\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "070450f2-2754-407f-81ab-211af81de44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    #A building block. Each layer is capable of performing two things:\n",
    "    #- Process input to get output:           output = layer.forward(input)\n",
    "    \n",
    "    #- Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    #Some layers also have learnable parameters which they update during layer.backward.\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Here we can initialize layer parameters (if any) and auxiliary stuff.\n",
    "        # A dummy layer does nothing\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n",
    "        \n",
    "        # A dummy layer just returns whatever it gets as input.\n",
    "        return input\n",
    "    def backward(self, input, grad_output):\n",
    "        # Performs a backpropagation step through the layer, with respect to the given input.\n",
    "        \n",
    "        # To compute loss gradients w.r.t input, we need to apply chain rule (backprop):\n",
    "        \n",
    "        # d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
    "        \n",
    "        # Luckily, we already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        # If our layer has parameters (e.g. dense layer), we also need to update them here using d loss / d layer\n",
    "        \n",
    "        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n",
    "        num_units = input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(num_units)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input) # chain rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a4c5c3e-9f89-4b47-98e7-16f4c058c457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        # ReLU layer simply applies elementwise rectified linear unit to all inputs\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Apply elementwise ReLU to [batch, input_units] matrix\n",
    "        relu_forward = np.maximum(0,input)\n",
    "        return relu_forward\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        # Compute gradient of loss w.r.t. ReLU input\n",
    "        relu_grad = input > 0\n",
    "        return grad_output*relu_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ae2943-644a-465d-b5f2-508941cc943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        # A dense layer is a layer which performs a learned affine transformation:\n",
    "        # f(x) = <W*x> + b\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.random.normal(loc=0.0, \n",
    "                                        scale = np.sqrt(2/(input_units+output_units)), \n",
    "                                        size = (input_units,output_units))\n",
    "        self.biases = np.zeros(output_units)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        # Perform an affine transformation:\n",
    "        # f(x) = <W*x> + b\n",
    "        \n",
    "        # input shape: [batch, input_units]\n",
    "        # output shape: [batch, output units]\n",
    "        \n",
    "        return np.dot(input,self.weights) + self.biases\n",
    "    \n",
    "    def backward(self,input,grad_output):\n",
    "        # compute d f / d x = d f / d dense * d dense / d x\n",
    "        # where d dense/ d x = weights transposed\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        \n",
    "        # compute gradient w.r.t. weights and biases\n",
    "        grad_weights = np.dot(input.T, grad_output)\n",
    "        grad_biases = grad_output.mean(axis=0)*input.shape[0]\n",
    "        \n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "        \n",
    "        # Here we perform a stochastic gradient descent step. \n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        \n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64464be7-4e02-455d-9308-11bff6f27791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    # Compute crossentropy from logits[batch,n_classes] and ids of correct answers\n",
    "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
    "    \n",
    "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "    \n",
    "    return xentropy\n",
    "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    # Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return (- ones_for_answers + softmax) / logits.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0a935-6726-49ee-9a2e-41198068cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def load_dataset(flatten=False):\n",
    "    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    # normalize x\n",
    "    X_train = X_train.astype(float) / 255.\n",
    "    X_test = X_test.astype(float) / 255.\n",
    "    # we reserve the last 10000 training examples for validation\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "    if flatten:\n",
    "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
    "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
    "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n",
    "## Let's look at some example\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0a0b1c-6fe6-4a86-8a3d-6526a6fcafd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
